#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π (DomRF + Avito + DomClick)
–î–æ–±–∞–≤–ª–µ–Ω–æ:
- –£—á–µ—Ç —É–∂–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (—É–¥–∞–ª—è—é—Ç—Å—è –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π)
- –í—ã–≤–æ–¥ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –∫–∞–∂–¥–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å intfloat/multilingual-e5-large
- –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, —Å—Ç–æ–ø-—Å–ª–æ–≤, –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤)
"""
import os
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from pymongo import MongoClient
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, util

os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

PROJECT_ROOT = Path(__file__).resolve().parent
UNIFIED_COLLECTION_NAME = "unified_houses"
_semantic_model = None


# ---------------------- –ú–û–î–ï–õ–¨ ---------------------- #
def get_semantic_model():
    global _semantic_model
    if _semantic_model is None:
        print("ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–Ω—É—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å...")
        _semantic_model = SentenceTransformer('intfloat/multilingual-e5-large')
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    return _semantic_model


# ---------------------- –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø ---------------------- #
PREFIXES = [
    '–∂–∫', '—Ç–æ–∫', '–∂–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å', '–∫–æ–º–ø–ª–µ–∫—Å –∂–∏–ª—ã—Ö –∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤', '–∫–æ–º–ø–ª–µ–∫—Å –∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤',
    '–∫–æ–º–ø–ª–µ–∫—Å –≤—ã—Å–æ—Ç–Ω—ã—Ö –¥–æ–º–æ–≤', '–∫–ª—É–±–Ω—ã–π –¥–æ–º', '–∫–ª—É–±–Ω–∞—è —Ä–µ–∑–∏–¥–µ–Ω—Ü–∏—è', '–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω',
    '—Å–µ–º–µ–π–Ω—ã–π –∫–≤–∞—Ä—Ç–∞–ª', '–∑–Ω–∞–∫–æ–≤—ã–π –∫–≤–∞—Ä—Ç–∞–ª', '–∫—Ä–∞—Å–æ—á–Ω—ã–π –∫–≤–∞—Ä—Ç–∞–ª', '–∂–∏–ª–æ–π –∫–≤–∞—Ä—Ç–∞–ª',
    '–∫–æ–º–ø–ª–µ–∫—Å', '–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã', '–∂–∏–ª–æ–π', '–∫–≤–∞—Ä—Ç–∞–ª', '–¥–æ–º'
]
STOP_WORDS = [
    '–ª–∏—Ç–µ—Ä', '–ª–∏—Ç–µ—Ä–∞', '—Å–µ–∫—Ü–∏—è', '—Å–µ–∫—Ü–∏–∏', '—ç—Ç–∞–ø', '–æ—á–µ—Ä–µ–¥—å',
    '–ø–∞—Ä–∫–∏–Ω–≥', '–∫–æ—Ä–ø—É—Å', '—Å—Ç—Ä–æ–µ–Ω–∏–µ', '–Ω–æ–º–µ—Ä', '‚Ññ'
]


def normalize_name(name: str) -> str:
    if not name:
        return ''
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    name = name.lower().strip()
    
    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    name = re.sub(r'[¬´¬ª"\'`\(\)\.,;:!?\-]', ' ', name)
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
    for prefix in PREFIXES:
        if name.startswith(prefix + ' '):
            name = name[len(prefix):].strip()
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    for word in STOP_WORDS:
        pattern = r'\b' + re.escape(word) + r'\b\s*'
        name = re.sub(pattern, '', name)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    name = re.sub(r'\s+', ' ', name).strip()
    
    return name


# ---------------------- –°–•–û–ñ–ï–°–¢–¨ ---------------------- #
def semantic_similarity(a: str, b: str, model) -> float:
    if not a or not b:
        return 0.0
    a, b = normalize_name(a), normalize_name(b)
    if not a or not b:
        return 0.0
    try:
        emb1, emb2 = model.encode([a, b], convert_to_tensor=True)
        score = util.cos_sim(emb1, emb2).item()
        return float(score)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏: {e}")
        return 0.0


# ---------------------- –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ï ---------------------- #
def find_best_match(source_name: str, target_records: List[Dict], get_name_func, model, threshold: float = 0.8):
    best_match, best_score = None, 0.0
    for rec in target_records:
        name = get_name_func(rec)
        if not name:
            continue
        score = semantic_similarity(source_name, name, model)
        if score > best_score:
            best_score = score
            best_match = rec
    if best_score >= threshold:
        print(f"‚úÖ –õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {get_name_func(best_match)} (—Å—Ö–æ–∂–µ—Å—Ç—å {best_score:.2f})")
        return best_match
    print(f"‚ùå –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–º–∞–∫—Å. {best_score:.2f})")
    return None


# ---------------------- –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---------------------- #
def create_unified_record(domrf: Dict, avito: Optional[Dict], domclick: Optional[Dict]) -> Dict:
    return {
        'source': 'unified',
        'domrf': {
            'name': domrf.get('objCommercNm'),
            'latitude': domrf.get('latitude'),
            'longitude': domrf.get('longitude')
        },
        'avito': avito,
        'domclick': domclick
    }


def load_env(parser_name: str) -> Dict[str, str]:
    env_path = PROJECT_ROOT / parser_name / '.env'
    if not env_path.exists():
        return {}
    load_dotenv(env_path, override=True)
    return {
        'MONGO_URI': os.getenv('MONGO_URI'),
        'DB_NAME': os.getenv('DB_NAME'),
        'COLLECTION_NAME': os.getenv('COLLECTION_NAME')
    }


def create_unified_collection():
    print("üöÄ –°–û–ó–î–ê–ù–ò–ï –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ô –ö–û–õ–õ–ï–ö–¶–ò–ò (E5-Large + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)")
    print("=" * 80)

    cfg = {p: load_env(p) for p in ['domrf', 'avito', 'domclick']}
    if not all(cfg[p] for p in cfg):
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ .env")
        return

    client = MongoClient(cfg['domrf']['MONGO_URI'])
    db = client[cfg['domrf']['DB_NAME']]

    domrf_col = db[cfg['domrf']['COLLECTION_NAME']]
    avito_col = db[cfg['avito']['COLLECTION_NAME']]
    domclick_col = db[cfg['domclick']['COLLECTION_NAME']]

    unified_col = db[UNIFIED_COLLECTION_NAME]
    unified_col.drop()

    model = get_semantic_model()

    domrf_records = list(domrf_col.find())
    avito_records = list(avito_col.find())
    domclick_records = list(domclick_col.find())

    matched_avito_ids, matched_domclick_ids = set(), set()

    for i, domrf in enumerate(domrf_records, 1):
        name = domrf.get('objCommercNm')
        if not name:
            continue
        print(f"\nüîç {i}/{len(domrf_records)} DomRF: {name}")

        avito_match = find_best_match(name, [r for r in avito_records if r['_id'] not in matched_avito_ids],
                                      lambda r: r.get('development', {}).get('name', ''), model)
        if not avito_match:
            continue
        matched_avito_ids.add(avito_match['_id'])

        domclick_match = find_best_match(name, [r for r in domclick_records if r['_id'] not in matched_domclick_ids],
                                         lambda r: r.get('development', {}).get('complex_name', ''), model)
        if not domclick_match:
            continue
        matched_domclick_ids.add(domclick_match['_id'])

        unified_col.insert_one(create_unified_record(domrf, avito_match, domclick_match))

    print(f"\n‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {unified_col.count_documents({})} –∑–∞–ø–∏—Å–µ–π")

    unmatched_avito = [r for r in avito_records if r['_id'] not in matched_avito_ids]
    unmatched_domclick = [r for r in domclick_records if r['_id'] not in matched_domclick_ids]

    print("\nüìä –ò—Ç–æ–≥ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:")
    print(f"  ‚Ä¢ Avito: {len(unmatched_avito)}")
    print(f"  ‚Ä¢ DomClick: {len(unmatched_domclick)}")

    if unmatched_avito:
        print("\nüìã –ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ Avito:")
        for r in unmatched_avito[:20]:
            print(f"   - {r.get('development', {}).get('name', '')}")
    if unmatched_domclick:
        print("\nüìã –ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ DomClick:")
        for r in unmatched_domclick[:20]:
            print(f"   - {r.get('development', {}).get('complex_name', '')}")

    client.close()


if __name__ == "__main__":
    create_unified_collection()
